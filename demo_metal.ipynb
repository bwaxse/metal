{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METAL\n"
]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "import polars as pl\n",
    "import scipy\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "conditions = [\"afib\", \"allergies\", \"asthma\", \"cholelithiasis\", \"t2d\", \"fibroids\"]\n",
    "categories = ['ehr', 'survey', 'ehr_and_survey', 'ehr_or_survey']\n",
    "\n",
    "traits = {}\n",
    "for disease in conditions:\n",
    "    for condition in categories:\n",
    "        traits[f\"{disease}_{condition}\"] = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output folders\n",
    "output_folder = f'{my_bucket}/data/saige_gwas/v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pl.read_csv(f'{output_folder}/amr/allergies_ehr/gwas_results.tsv.gz', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_file_exists(gs_path):\n",
    "    \"\"\"Check if specific GCS file exists using gsutil ls\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(f'gsutil ls {gs_path}', \n",
    "                              shell=True, capture_output=True)\n",
    "        return result.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def validate_saige_inputs(trait, ancestries, base_output_folder):\n",
    "    \"\"\"\n",
    "    Validate SAIGE output files for METAL meta-analysis\n",
    "    \"\"\"\n",
    "    required_file = \"gwas_results.tsv.gz\"\n",
    "    validated_inputs = {}\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        in_dir = f\"{base_output_folder}/{anc}/{trait}\"\n",
    "        file_path = f\"{in_dir}/{required_file}\"\n",
    "    \n",
    "        # Check file exists\n",
    "        if not gcs_file_exists(file_path):\n",
    "            print(f\"Missing {required_file} for ancestry {anc}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract sample sizes from the file\n",
    "        sample_info = extract_sample_sizes(file_path)\n",
    "        if not sample_info:\n",
    "            print(f\"Could not extract sample sizes for ancestry {anc}\")\n",
    "            continue\n",
    "\n",
    "        validated_inputs[anc] = {\n",
    "            'path': file_path,\n",
    "            'n_cases': sample_info['n_cases'],\n",
    "            'n_controls': sample_info['n_controls'], \n",
    "            'n_total': sample_info['n_total']\n",
    "        }\n",
    "        \n",
    "        print(f\"Ancestry {anc}: {sample_info['n_cases']} cases, {sample_info['n_controls']} controls (total: {sample_info['n_total']})\")\n",
    "    \n",
    "    if len(validated_inputs) < 2:\n",
    "        print(f\"Error: Need at least 2 ancestries for meta-analysis. Found: {len(validated_inputs)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Validated {len(validated_inputs)} studies for {trait} \\n\")    \n",
    "    return validated_inputs\n",
    "\n",
    "def extract_sample_sizes(file_path):\n",
    "    \"\"\"\n",
    "    Extract n_cases and n_controls from first data row of SAIGE file\n",
    "    Returns dict with sample size info or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read just the first few lines to get sample sizes\n",
    "        cmd = f\"gsutil cat '{file_path}' | gunzip | head -2 2>/dev/null\"\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if not result.stdout:\n",
    "            print(f\"No output from command for {file_path}\")\n",
    "            return None\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "\n",
    "        if len(lines) < 2:\n",
    "            print(f\"File {file_path} has insufficient data\")\n",
    "            return None\n",
    "            \n",
    "        header = lines[0].split('\\t')\n",
    "        data = lines[1].split('\\t')\n",
    "        \n",
    "        # Find column indices\n",
    "        try:\n",
    "            n_cases_idx = header.index('n_cases')\n",
    "            n_controls_idx = header.index('n_controls')\n",
    "        except ValueError as e:\n",
    "            print(f\"Missing required columns in {file_path}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract values\n",
    "        n_cases = int(float(data[n_cases_idx]))\n",
    "        n_controls = int(float(data[n_controls_idx]))\n",
    "        n_total = n_cases + n_controls\n",
    "        \n",
    "        return {\n",
    "            'n_cases': n_cases,\n",
    "            'n_controls': n_controls,\n",
    "            'n_total': n_total\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sample sizes from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_dsub_readable(cmd):\n",
    "    \"\"\"\n",
    "    Simple readable format - newline before each --\n",
    "    \"\"\"\n",
    "    readable_cmd = cmd.replace(' --', ' \\\\\\n    --')\n",
    "    print(readable_cmd)\n",
    "    print()  # Extra line for separation\n",
    "    \n",
    "def dsub_script(\n",
    "    label,\n",
    "    machine_type,\n",
    "    envs,\n",
    "    in_params,\n",
    "    out_params,\n",
    "    out_dirs,\n",
    "    boot_disk = 100,\n",
    "    disk_size = 150,\n",
    "    image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script = 'run_metal.sh',\n",
    "    preemptible = True\n",
    "):\n",
    "    \n",
    "    # get useful info\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.','-')\n",
    "\n",
    "    job_name = f'{label}'\n",
    "    \n",
    "    dsub_cmd = 'dsub '\n",
    "    dsub_cmd += '--provider google-batch '\n",
    "    dsub_cmd += '--user-project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--image \"{}\" '.format(image)\n",
    "    dsub_cmd += '--network \"global/networks/network\" '\n",
    "    dsub_cmd += '--subnetwork \"regions/us-central1/subnetworks/subnetwork\" '\n",
    "    dsub_cmd += '--service-account \"$(gcloud config get-value account)\" '\n",
    "    dsub_cmd += '--use-private-address '\n",
    "    dsub_cmd += '--user \"{}\" '.format(dsub_user_name)\n",
    "    dsub_cmd += '--regions us-central1 '\n",
    "    dsub_cmd += '--logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +\\'%Y%m%d\\')/{job-id}-{task-id}-{task-attempt}.log\" '\n",
    "    dsub_cmd += ' \"$@\" '\n",
    "    dsub_cmd += '--name \"{}\" '.format(job_name)\n",
    "    dsub_cmd += '--machine-type \"{}\" '.format(machine_type)\n",
    "    \n",
    "    if preemptible:\n",
    "        dsub_cmd += '--preemptible '\n",
    "        \n",
    "    if 'c4' in machine_type:\n",
    "        raise ValueError(\n",
    "            f\"c4 machine types ('{machine_type}') are not supported with dsub. \"\n",
    "            f\"c4 requires hyperdisk-balanced boot disks, but dsub doesn't allow \"\n",
    "            f\"setting boot disks. Use c2 or n2 instead.\"\n",
    "        )\n",
    "\n",
    "#        # c4 doesn't use pd-ssd\n",
    "#         dsub_cmd += '--disk-type \"hyperdisk-balanced\" '\n",
    "#     else:\n",
    "#         dsub_cmd += '--disk-type \"pd-ssd\" '\n",
    "        \n",
    "    dsub_cmd += '--boot-disk-size {} '.format(boot_disk)\n",
    "    dsub_cmd += '--disk-size {} '.format(disk_size)\n",
    "    dsub_cmd += '--script \"{}\" '.format(script)\n",
    "    \n",
    "    # Assign any environmental conditions\n",
    "    for env_key in envs.keys():\n",
    "        dsub_cmd += '--env {}=\"{}\" '.format(env_key, envs[env_key])\n",
    "        \n",
    "    # Assign any inputs\n",
    "    for in_key in in_params.keys():\n",
    "        dsub_cmd += '--input {}=\"{}\" '.format(in_key, in_params[in_key])\n",
    "        \n",
    "    # Assign any outputs\n",
    "    if out_params != None:\n",
    "        for out_key in out_params.keys():\n",
    "            dsub_cmd += '--output {}=\"{}\" '.format(out_key, out_params[out_key])\n",
    "        \n",
    "    for out_key in out_dirs.keys():\n",
    "        dsub_cmd += '--output-recursive {}=\"{}\" '.format(out_key, out_dirs[out_key])\n",
    "\n",
    "    os.system(dsub_cmd)\n",
    "#     print_dsub_readable(dsub_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_age_format(age: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate age format for dsub dstat command\n",
    "    \"\"\"\n",
    "    # Pattern: one or more digits followed by exactly one valid unit\n",
    "    pattern = r'^\\d+[smhdw]$'\n",
    "    return bool(re.match(pattern, age.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user: str = None, full: bool = False, age: str = '1d') -> subprocess.CompletedProcess:\n",
    "    \"\"\"\n",
    "    Check status of dsub jobs for the specified user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user : str, optional\n",
    "        Username to check jobs for. Defaults to current user from OWNER_EMAIL\n",
    "    full : bool, default False\n",
    "        Include full job details in output\n",
    "    age : str, default '1d'\n",
    "        Maximum age of jobs to display. Format: <integer><unit>\n",
    "        Units: s (seconds), m (minutes), h (hours), d (days), w (weeks)\n",
    "        Examples: '3d', '12h', '30m', '7w'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    subprocess.CompletedProcess\n",
    "        Result of the dstat command\n",
    "        \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> check_dsub_status(age='3d', full=True)  # Last 3 days, full details\n",
    "    >>> check_dsub_status()  # Default: last day, summary view\n",
    "    \"\"\"\n",
    "    \n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Validate age parameter\n",
    "    if age is not None:\n",
    "        if not validate_age_format(age):\n",
    "            raise ValueError(\n",
    "                f\"Invalid age format: '{age}'. \"\n",
    "                \"Expected format: <integer><unit> where unit is one of: s, m, h, d, w. \"\n",
    "                \"Examples: '3d', '12h', '30m', '7w'\"\n",
    "            )\n",
    "    \n",
    "    # Build command\n",
    "    cmd_parts = [\n",
    "        \"dstat\",\n",
    "        \"--provider google-batch\",\n",
    "        f\"--user {user}\",\n",
    "        \"--status '*'\",\n",
    "        f\"--project {project}\"\n",
    "    ]\n",
    "    \n",
    "    if full:\n",
    "        cmd_parts.append(\"--full\")\n",
    "    \n",
    "    if age:\n",
    "        cmd_parts.append(f\"--age {age}\")\n",
    "    \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-batch --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-batch --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_dsub_logs(log_path):\n",
    "    base_path = log_path.replace('.log', '')\n",
    "    \n",
    "    print(\"=== STDOUT ===\")\n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stdout.log'])\n",
    "    \n",
    "    print(\"\\n=== STDERR ===\") \n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stderr.log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_metal.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_metal.sh\n",
    "\n",
    "#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Starting METAL meta-analysis for ${TRAIT}\"\n",
    "echo \"Number of input files: ${N_FILES}\"\n",
    "\n",
    "# Parse ancestries\n",
    "IFS=',' read -ra INPUTS <<< \"${INPUTS}\"\n",
    "\n",
    "# Parse sample sizes\n",
    "IFS=',' read -ra SIZES <<< \"${SAMPLE_SIZES}\"\n",
    "\n",
    "echo \"Preprocessing files to add ancestry-specific n_total columns...\"\n",
    "PROCESSED_FILES=()\n",
    "\n",
    "# Pre-processing adds sample size via n_cases and n_controls extracted from SAIGE GWAS results \n",
    "for i in \"${!INPUTS[@]}\"; do\n",
    "    var_name=\"${INPUTS[$i]}\"\n",
    "    input_file=\"${!var_name}\"\n",
    "    n_total=\"${SIZES[$i]}\"\n",
    "    processed_file=\"/tmp/processed_file_${i}.tsv\"\n",
    "    \n",
    "    echo \"Processing file $((i+1)): ${input_file} (n_total=${n_total})\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if [[ ! -f \"${input_file}\" ]]; then\n",
    "        echo \"ERROR: Input file not found: ${input_file}\"\n",
    "        echo \"Available files in /mnt/data/input/:\"\n",
    "        ls -la /mnt/data/input/ || echo \"Input directory not accessible\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Add constant n_total column for this ancestry\n",
    "    if [[ \"${input_file}\" == *.gz ]]; then\n",
    "        gzip -dc \"${input_file}\"\n",
    "    else\n",
    "        cat \"${input_file}\"\n",
    "    fi | awk -F'\\t' -v OFS='\\t' -v n_total=\"${n_total}\" '\n",
    "        NR==1 { print $0, \"n_total\"; next }\n",
    "               { print $0, n_total }' > \"${processed_file}\"\n",
    "    \n",
    "    PROCESSED_FILES+=(\"/processed_file_${i}.tsv\")\n",
    "    \n",
    "    # Verify output\n",
    "    n_lines=$(wc -l < \"${processed_file}\")\n",
    "    echo \"  Created ${processed_file} with ${n_lines} lines\"\n",
    "done\n",
    "\n",
    "# Convert processed files array to comma-separated string\n",
    "PROCESSED_FILES_STR=$(IFS=','; echo \"${PROCESSED_FILES[*]}\")\n",
    "echo \"Running METAL with processed files: ${PROCESSED_FILES_STR}\"\n",
    "\n",
    "# Create a METAL script dynamically\n",
    "METAL_SCRIPT=\"${OUTPUT_PATH}/metal_script.txt\"\n",
    "cat > \"${METAL_SCRIPT}\" <<EOF\n",
    "MARKER vid\n",
    "WEIGHT n_total\n",
    "ALLELE effect_allele other_allele\n",
    "FREQ effect_allele_frequency\n",
    "PVAL p_value\n",
    "EFFECT beta\n",
    "STDERR standard_error\n",
    "SEPARATOR TAB\n",
    "SCHEME STDERR\n",
    "\n",
    "# Auto-flip alleles based on frequency\n",
    "AVERAGEFREQ ON\n",
    "MINMAXFREQ ON\n",
    "\n",
    "$(for file in \"${PROCESSED_FILES[@]}\"; do echo \"PROCESS $file\"; done)\n",
    "\n",
    "OUTFILE ${OUT_PREF} .tsv\n",
    "ANALYZE HETEROGENEITY\n",
    "QUIT\n",
    "EOF\n",
    "\n",
    "echo \"Metal script created\"\n",
    "\n",
    "# Run METAL\n",
    "metal \"${METAL_SCRIPT}\"\n",
    "\n",
    "mv ${OUT_PREF}1.tsv \"${OUTPUT_PATH}/\"\n",
    "mv ${OUT_PREF}1.tsv.info \"${OUTPUT_PATH}/\"\n",
    "\n",
    "# List all output files\n",
    "echo \"Final contents of OUTPUT_PATH (${OUTPUT_PATH}):\"\n",
    "ls -lh \"${OUTPUT_PATH}\" || echo \"OUTPUT_PATH not accessible\"\n",
    "\n",
    "echo \"METAL analysis completed successfully for ${TRAIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metal(trait, ancestries, base_output_folder):\n",
    "    \"\"\"\n",
    "    Run METAL meta-analysis with SAIGE-formatted inputs\n",
    "    \"\"\"\n",
    "    artifact_registry = os.getenv('ARTIFACT_REGISTRY_DOCKER_REPO', '')\n",
    "            \n",
    "    # Validate SAIGE inputs\n",
    "    validated_inputs = validate_saige_inputs(trait, ancestries, base_output_folder)\n",
    "    if not validated_inputs:\n",
    "        return None\n",
    "\n",
    "    out_dir = f'{base_output_folder}/metal/{trait}'\n",
    "\n",
    "    # METAL configuration\n",
    "    env_dict = {\n",
    "        'TRAIT': trait,\n",
    "        'OUT_PREF': trait,\n",
    "    }\n",
    "\n",
    "    in_dict = {}\n",
    "    file_list = []\n",
    "    sample_sizes = []\n",
    "    \n",
    "    for i, (anc, file_info) in enumerate(validated_inputs.items(), 1):\n",
    "        in_dict[f'INPUT_{anc.upper()}'] = file_info['path']\n",
    "        file_list.append(file_info['path'])\n",
    "        sample_sizes.append(str(file_info['n_total']))\n",
    "    \n",
    "    # Pass file list and sample sizes as comma-separated strings\n",
    "    env_dict['INPUTS'] = \",\".join(in_dict.keys())\n",
    "    env_dict['SAMPLE_SIZES'] = ','.join(sample_sizes)  # n_total per ancestry\n",
    "    env_dict['N_FILES'] = str(len(file_list))\n",
    "\n",
    "    out_dirs = {\n",
    "        'OUTPUT_PATH': out_dir\n",
    "    }\n",
    "\n",
    "    return dsub_script(\n",
    "        label=f'metal_{trait}',\n",
    "        machine_type='n2d-standard-8',\n",
    "        envs=env_dict,\n",
    "        in_params=in_dict,\n",
    "        out_params=None,\n",
    "        out_dirs=out_dirs,\n",
    "        boot_disk=100,\n",
    "        disk_size=150,\n",
    "        image=f'{artifact_registry}/bwaxse/metal',\n",
    "        script='run_metal.sh',\n",
    "        preemptible=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancestry eas: 643 cases, 4491 controls (total: 5134)\n",
      "Ancestry amr: 3634 cases, 37905 controls (total: 41539)\n",
      "Validated 2 studies for allergies_ehr \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job properties:\n",
      "  job-id: metal-alle--bwaxse--xxxx-xxxxx\n",
      "  job-name: metal-allergies-ehr\n",
      "  user-id: bwaxse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metal-alle--bwaxse--xxxx-xxxxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provider internal-id (operation): projects/project/locations/us-central1/jobs/metal-alle--bwaxse--xxxx-xxxxx-0-0\n",
      "Launched job-id: metal-alle--bwaxse--xxxx-xxxxx\n",
      "To check the status, run:\n",
      "  dstat --provider google-batch --project project --location us-central1 --jobs 'metal-alle--bwaxse--xxxx-xxxxx' --users 'bwaxse' --status '*'\n",
      "To cancel the job, run:\n",
      "  ddel --provider google-batch --project project --location us-central1 --jobs 'metal-alle--bwaxse--xxxx-xxxxx' --users 'bwaxse'\n"
     ]
    }
   ],
   "source": [
    "# Ancestry-specific null model\n",
    "for trait in ['allergies_ehr']:\n",
    "    _ = run_metal(\n",
    "        trait=trait,\n",
    "        ancestries=['eas', 'amr'], #'amr', 'afr', 'eur', \n",
    "        base_output_folder=output_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: dstat --provider google-batch --user bwaxse --status '*' --project project --age 1d\n",
      "Job Name         Status                                      Last Update\n",
      "---------------  ------------------------------------------  --------------\n",
      "metal-allerg...  Job state is set from RUNNING to SUCCEE...  08-26 19:03:40\n",
      "s2-eur-7-all...  Job state is set from SCHEDULED to RUNN...  08-26 18:53:01\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"dstat --provider google-batch --user bwaxse --status '*' --project project --age 1d\", returncode=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[output removed]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='dstat --provider google-batch --project project --user bwaxse --status --jobs metal-alle--bwaxse--xxxx-xxxxx --full', returncode=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_details(job='metal-alle--bwaxse--xxxx-xxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=\"{my_bucket}/dsub/logs/metal-allergies-ehr/bwaxse/20250826/metal-alle--bwaxse--xxxx-xxxxx-task-None.log\"\n",
    "view_dsub_logs(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{my_bucket}/data/saige_gwas/v0/metal/allergies_ehr/allergies_ehr1.tsv\r\n",
      "{my_bucket}/data/saige_gwas/v0/metal/allergies_ehr/allergies_ehr1.tsv.info\r\n",
      "{my_bucket}/data/saige_gwas/v0/metal/allergies_ehr/metal_script.txt\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {my_bucket}/data/saige_gwas/v0/metal/allergies_ehr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_running_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "# project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "# # Cancel only running jobs\n",
    "# cancel_cmd = f\"ddel --provider google-batch --project {project} --users 'bwaxse' --jobs 'metal-alle--bwaxse--xxxxx-xxxxx'\"\n",
    "# print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "\n",
    "# subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
